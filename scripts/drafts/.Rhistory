set.seed(110122)
train_task2 <- training(partitions_word_tokenized) %>%
select(-.id, -bclass)
train_labels_task2 <- training(partitions_word_tokenized) %>%
select(.id, bclass)
set.seed(110122)
test_task2 <- testing(partitions_word_tokenized) %>%
select(-.id, -bclass)
test_labels_task2 <- testing(partitions_word_tokenized) %>%
select(.id, bclass)
# find projections based on training data
set.seed(110122)
proj_out_task2 <- projection_fn(.dtm = train_task2, .prop = 0.7)
train_projected_task2 <- proj_out_task2$data
# We're fitting the new word tokenized PC logistic regression model
train_claim_task2 <- train_labels_task2 %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(train_projected_task2)
fit_claim_task2 <- glm(bclass ~ ., data = train_claim_task2, family = "binomial") # warning: evidence of overfitting
# log-odds-ratios are the outputs of the logistic regression model without the link function that maps the response to either 0 or 1
log_odds_ratios_train <- predict(fit_claim_task2, newdata = train_claim_task2, type = "link")
### w/o headers
# project test data onto PCs
test_projected_task2 <- reproject_fn(.dtm = test_task2, proj_out_task2)
# coerce to matrix
#x_test <- as.matrix(test_projected_task2)
# compute predicted probabilities
preds <- predict(fit_claim_task2,
test_projected_task2,
type = 'response')
# store predictions in a data frame with true labels
pred_df_task2 <- test_labels_task2 %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(pred = as.numeric(preds)) %>%
mutate(bclass.pred = factor(pred > 0.5,
labels = levels(bclass)))
# define classification metric panel
panel <- metric_set(sensitivity,
specificity,
accuracy,
roc_auc)
# compute test set accuracy
pred_df_task2 %>% panel(truth = bclass,
estimate = bclass.pred,
pred,
event_level = 'second')
# partition data
set.seed(110122)
partitions_bigram <- claims_bigram_tfidf %>% initial_split(prop = 0.7)
# train/test split bigram
set.seed(110122)
train_bigram <- training(partitions_bigram) %>%
select(-.id, -bclass)
train_bigram_labels <- training(partitions_bigram) %>%
select(.id, bclass)
set.seed(110122)
test_bigram <- testing(partitions_bigram) %>%
select(-.id, -bclass)
test_bigram_labels <- testing(partitions_bigram) %>%
select(.id, bclass)
# find projections based on training data
proj_out_bigram <- projection_fn(.dtm = train_bigram, .prop = 0.7)
train_projected_bigram <- proj_out_bigram$data
train_claim_bigram <- train_bigram_labels %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(train_projected_bigram)
# adding log-odds-ratios to the projected bigram training data
train_claim_bigram <- cbind(train_claim_bigram[, 1], log_odds_ratios_train, train_claim_bigram[, -1])
names(train_claim_bigram)[2] <- 'log_odds_ratio'
train_claim_bigram
fit_claim_bigram <- glm(bclass ~ ., data = train_claim_bigram, family = "binomial") # warning: evidence of overfitting
# finding log-odds-ratios on projected testing data
log_odds_ratios_test <- predict(fit_claim_task2, newdata = test_projected_task2, type = "link")
# project test data onto PCs
test_projected_bigram <- reproject_fn(.dtm = test_bigram, proj_out_bigram)
# adding log-odds-ratios to the projected bigram training data
test_claim_bigram <- cbind(log_odds_ratios_test, test_projected_bigram)
names(test_claim_bigram)[1] <- 'log_odds_ratio'
test_claim_bigram
# coerce to matrix
#x_test <- as.matrix(test_projected)
# compute predicted probabilities
preds_bigram <- predict(fit_claim_bigram, test_claim_bigram, type = 'response')
# store predictions in a data frame with true labels
pred_df_bigram <- test_bigram_labels %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(pred = as.numeric(preds_bigram)) %>%
mutate(bclass.pred = factor(pred > 0.5,
labels = levels(bclass)))
# define classification metric panel
panel <- metric_set(sensitivity,
specificity,
accuracy,
roc_auc)
# compute test set accuracy
pred_df_bigram %>% panel(truth = bclass,
estimate = bclass.pred,
pred,
event_level = 'second')
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
library(tokenizers)
library(textstem)
library(stopwords)
library(tidyverse)
library(tidymodels)
library(modelr)
library(Matrix)
library(sparsesvd)
library(glmnet)
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/activities/data/'
# load a few functions for the activity
source(paste(url, 'projection-functions.R', sep = ''))
root_dir <- rprojroot::find_rstudio_root_file()
data_dir <- file.path(root_dir, "data")
scripts_dir <- file.path(root_dir, "scripts")
results_dir <- file.path(root_dir, "results")
setwd(data_dir)
load("claims-clean-example.RData")
load("claims-raw.RData")
load("claims-clean-header.Rdata")
log_odds_ratio <- exp(summary(fit_claim)$coef[,1])
# store predictions in a data frame with true labels
pred_header_df <- test_header_labels %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(pred = as.numeric(preds_header)) %>%
mutate(bclass.pred = factor(pred > 0.5,
labels = levels(bclass)))
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
library(tokenizers)
library(textstem)
library(stopwords)
library(tidyverse)
library(tidymodels)
library(modelr)
library(Matrix)
library(sparsesvd)
library(glmnet)
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/activities/data/'
# load a few functions for the activity
source(paste(url, 'projection-functions.R', sep = ''))
root_dir <- rprojroot::find_rstudio_root_file()
data_dir <- file.path(root_dir, "data")
scripts_dir <- file.path(root_dir, "scripts")
results_dir <- file.path(root_dir, "results")
setwd(data_dir)
load("claims-clean-example.RData")
load("claims-raw.RData")
load("claims-clean-header.Rdata")
setwd(scripts_dir)
source('preprocessing.R')
# tf-idf matrix for claims w/o headers
claims_tfidf <- nlp_fn(claims_clean)
# tf-idf matrix for claims w/ headers
claims_header_tfidf <- nlp_fn(claims_clean_header)
# partition data
set.seed(110122)
partitions <- claims_tfidf %>% initial_split(prop = 0.7)
partitions_header <- claims_header_tfidf %>% initial_split(prop = 0.7)
# train/test split w/o headers
train <- training(partitions) %>%
select(-.id, -bclass)
train_labels <- training(partitions) %>%
select(.id, bclass)
test <- testing(partitions) %>%
select(-.id, -bclass)
test_labels <- testing(partitions) %>%
select(.id, bclass)
# train/test split w/ headers
train_header <- training(partitions_header) %>%
select(-.id, -bclass)
train_header_labels <- training(partitions_header) %>%
select(.id, bclass)
test_header <- testing(partitions_header) %>%
select(-.id, -bclass)
test_header_labels <- testing(partitions_header) %>%
select(.id, bclass)
### w/o headers
# find projections based on training data
proj_out <- projection_fn(.dtm = train, .prop = 0.7)
train_projected <- proj_out$data
### w/ headers
# find projections based on training data
proj_out_header <- projection_fn(.dtm = train_header, .prop = 0.7)
train_projected_header <- proj_out_header$data
### w/o headers
train_claim <- train_labels %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(train_projected)
fit_claim <- glm(bclass ~ ., data = train_claim, family = "binomial") # warning: evidence of overfitting
### w/ headers
train_claim_header <- train_header_labels %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(train_projected_header)
fit_claim_header <- glm(bclass ~ ., data = train_claim_header, family = "binomial") # warning: evidence of overfitting
### w/o headers
# project test data onto PCs
test_projected <- reproject_fn(.dtm = test, proj_out)
# coerce to matrix
x_test <- as.matrix(test_projected)
# compute predicted probabilities
preds <- predict(fit_claim,
test_projected,
type = 'response')
### w/ headers
# project test data onto PCs
test_projected_header <- reproject_fn(.dtm = test_header, proj_out_header)
# coerce to matrix
x_test_header <- as.matrix(test_projected_header)
# compute predicted probabilities
preds_header <- predict(fit_claim_header,
test_projected_header,
type = 'response')
# store predictions in a data frame with true labels
pred_df <- test_labels %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(pred = as.numeric(preds)) %>%
mutate(bclass.pred = factor(pred > 0.5,
labels = levels(bclass)))
# define classification metric panel
panel <- metric_set(sensitivity,
specificity,
accuracy,
roc_auc)
# compute test set accuracy
pred_df %>% panel(truth = bclass,
estimate = bclass.pred,
pred,
event_level = 'second')
# store predictions in a data frame with true labels
pred_header_df <- test_header_labels %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(pred = as.numeric(preds_header)) %>%
mutate(bclass.pred = factor(pred > 0.5,
labels = levels(bclass)))
# compute test set accuracy
pred_header_df %>% panel(truth = bclass,
estimate = bclass.pred,
pred,
event_level = 'second')
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
library(tokenizers)
library(textstem)
library(stopwords)
library(tidyverse)
library(tidymodels)
library(modelr)
library(Matrix)
library(sparsesvd)
library(glmnet)
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/activities/data/'
# load a few functions for the activity
source(paste(url, 'projection-functions.R', sep = ''))
root_dir <- rprojroot::find_rstudio_root_file()
data_dir <- file.path(root_dir, "data")
scripts_dir <- file.path(root_dir, "scripts")
results_dir <- file.path(root_dir, "results")
setwd(data_dir)
load("claims-clean-example.RData")
load("claims-raw.RData")
load("claims-clean-header.Rdata")
log_odds_ratio <- exp(summary(fit_claim)$coef[,1])
log_odds_ratio
# Rotate, if dimensions dont match remove intercept
pivotted_logodds<-as.data.frame(log_odds_ratio)
dim(pivotted_logodds)
# cbind to pca (choose first couple bigram pca because its usually in order of significance)
setwd(scripts_dir)
source('preprocessing.R')
claims_bigram_tfidf <- bigram_fn1(claims_clean)
claims_bigram_tfidf
# partition data
set.seed(110122)
partitions <- claims_bigram_tfidf %>% initial_split(prop = 0.7)
# train/test split bigram
train_bigram <- training(partitions) %>%
select(-.id, -bclass)
train_bigram_labels <- training(partitions) %>%
select(.id, bclass)
test_bigram <- testing(partitions) %>%
select(-.id, -bclass)
test_bigram_labels <- testing(partitions) %>%
select(.id, bclass)
# find projections based on training data
proj_out_bigram <- projection_fn(.dtm = train_bigram, .prop = 0.7)
train_projected_bigram <- proj_out_bigram$data
train_claim_bigram <- train_bigram_labels %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(train_projected_bigram)
fit_claim_bigram <- glm(bclass ~ ., data = train_claim_bigram, family = "binomial") # warning: evidence of overfitting
bigram_pcas<-as.data.frame(train_claim_bigram)
# Combine Log-Odds and Bigrams
pivotted_logodds
# project test data onto PCs
test_projected_bigram <- reproject_fn(.dtm = test_bigram, proj_out_bigram)
# coerce to matrix
#x_test <- as.matrix(test_projected)
# compute predicted probabilities
preds_bigram <- predict(fit_claim_bigram,
test_projected_bigram,
type = 'response')
# store predictions in a data frame with true labels
pred_df_bigram <- test_bigram_labels %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(pred = as.numeric(preds_bigram)) %>%
mutate(bclass.pred = factor(pred > 0.5,
labels = levels(bclass)))
# define classification metric panel
panel <- metric_set(sensitivity,
specificity,
accuracy,
roc_auc)
# compute test set accuracy
pred_df_bigram %>% panel(truth = bclass,
estimate = bclass.pred,
pred,
event_level = 'second')
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
library(tokenizers)
library(textstem)
library(stopwords)
library(tidyverse)
library(tidymodels)
library(modelr)
library(Matrix)
library(sparsesvd)
library(glmnet)
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/activities/data/'
# load a few functions for the activity
source(paste(url, 'projection-functions.R', sep = ''))
root_dir <- rprojroot::find_rstudio_root_file()
data_dir <- file.path(root_dir, "data")
scripts_dir <- file.path(root_dir, "scripts")
results_dir <- file.path(root_dir, "results")
setwd(data_dir)
load("claims-clean-example.RData")
load("claims-raw.RData")
load("claims-clean-header.Rdata")
#ADDED A COMMENT TO TEST PUSHING STUFF
setwd(scripts_dir)
source('preprocessing.R')
claims_bigram_tfidf <- bigram_fn1(claims_clean)
claims_tfidf <- nlp_fn(claims_clean)
task2_word_tfidf <- claims_tfidf[claims_tfidf$.id %in% claims_bigram_tfidf$.id, ] # makes sure that all data used to train bigrams is in word tokenized data
task2_word_tfidf
#train_labels
# partition data
set.seed(110122)
partitions_word_tokenized <- task2_word_tfidf %>% initial_split(prop = 0.7)
# train/test split w/o headers
set.seed(110122)
train_task2 <- training(partitions_word_tokenized) %>%
select(-.id, -bclass)
train_labels_task2 <- training(partitions_word_tokenized) %>%
select(.id, bclass)
set.seed(110122)
test_task2 <- testing(partitions_word_tokenized) %>%
select(-.id, -bclass)
test_labels_task2 <- testing(partitions_word_tokenized) %>%
select(.id, bclass)
# find projections based on training data
set.seed(110122)
proj_out_task2 <- projection_fn(.dtm = train_task2, .prop = 0.7)
train_projected_task2 <- proj_out_task2$data
# We're fitting the new word tokenized PC logistic regression model
train_claim_task2 <- train_labels_task2 %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(train_projected_task2)
fit_claim_task2 <- glm(bclass ~ ., data = train_claim_task2, family = "binomial") # warning: evidence of overfitting
# log-odds-ratios are the outputs of the logistic regression model without the link function that maps the response to either 0 or 1
log_odds_ratios_train <- predict(fit_claim_task2, newdata = train_claim_task2, type = "link")
### w/o headers
# project test data onto PCs
test_projected_task2 <- reproject_fn(.dtm = test_task2, proj_out_task2)
# coerce to matrix
#x_test <- as.matrix(test_projected_task2)
# compute predicted probabilities
preds <- predict(fit_claim_task2,
test_projected_task2,
type = 'response')
# store predictions in a data frame with true labels
pred_df_task2 <- test_labels_task2 %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(pred = as.numeric(preds)) %>%
mutate(bclass.pred = factor(pred > 0.5,
labels = levels(bclass)))
# define classification metric panel
panel <- metric_set(sensitivity,
specificity,
accuracy,
roc_auc)
# compute test set accuracy
pred_df_task2 %>% panel(truth = bclass,
estimate = bclass.pred,
pred,
event_level = 'second')
# partition data
set.seed(110122)
partitions_bigram <- claims_bigram_tfidf %>% initial_split(prop = 0.7)
# train/test split bigram
set.seed(110122)
train_bigram <- training(partitions_bigram) %>%
select(-.id, -bclass)
train_bigram_labels <- training(partitions_bigram) %>%
select(.id, bclass)
set.seed(110122)
test_bigram <- testing(partitions_bigram) %>%
select(-.id, -bclass)
test_bigram_labels <- testing(partitions_bigram) %>%
select(.id, bclass)
# find projections based on training data
proj_out_bigram <- projection_fn(.dtm = train_bigram, .prop = 0.7)
train_projected_bigram <- proj_out_bigram$data
train_claim_bigram <- train_bigram_labels %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(train_projected_bigram)
# adding log-odds-ratios to the projected bigram training data
train_claim_bigram <- cbind(train_claim_bigram[, 1], log_odds_ratios_train, train_claim_bigram[, -1])
names(train_claim_bigram)[2] <- 'log_odds_ratio'
train_claim_bigram
fit_claim_bigram <- glm(bclass ~ log_odds_ratio + pc1 + pc2 +pc3, data = train_claim_bigram, family = "binomial") # warning: evidence of overfitting
# finding log-odds-ratios on projected testing data
log_odds_ratios_test <- predict(fit_claim_task2, newdata = test_projected_task2, type = "link")
# project test data onto PCs
test_projected_bigram <- reproject_fn(.dtm = test_bigram, proj_out_bigram)
# adding log-odds-ratios to the projected bigram training data
test_claim_bigram <- cbind(log_odds_ratios_test, test_projected_bigram)
names(test_claim_bigram)[1] <- 'log_odds_ratio'
test_claim_bigram
# compute predicted probabilities
preds_bigram <- predict(fit_claim_bigram, test_claim_bigram, type = 'response')
# store predictions in a data frame with true labels
pred_df_bigram <- test_bigram_labels %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(pred = as.numeric(preds_bigram)) %>%
mutate(bclass.pred = factor(pred > 0.5,
labels = levels(bclass)))
# define classification metric panel
panel <- metric_set(sensitivity,
specificity,
accuracy,
roc_auc)
# compute test set accuracy
pred_df_bigram %>% panel(truth = bclass,
estimate = bclass.pred,
pred,
event_level = 'second')
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(tensorflow)
library(reticulate)
path_to_python <- "/Users/aarya_kulkarni/opt/anaconda3/bin/python3"
virtualenv_create("r-reticulate", python = path_to_python)
install_tensorflow(method = "conda", extra_packages = "tensorflow-macos", envname = "r-reticulate")
install_tensorflow(version = '2.13', envname = "r-reticulate")
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(tensorflow)
library(reticulate)
path_to_python <- "/Users/aarya_kulkarni/opt/anaconda3/bin/python"
virtualenv_create("r-reticulate", python = path_to_python)
install.packages("tensorflow")
install.packages("keras")
library(keras)
library(tensorflow)
install.packages("tensorflow")
knitr::opts_chunk$set(echo = TRUE)
install.packages("keras")
library(keras)
library(tensorflow)
tf$constant('Hello world')
library(keras)
library(tensorflow)
library(reticulate)
path_to_python <- "/Users/aarya_kulkarni/tensorflow-test/env/bin/python"
virtualenv_create("r-reticulate", python = path_to_python)
install_tensorflow(envname = "r-reticulate")
knitr::opts_chunk$set(echo = TRUE)
path_to_python <- "/Users/aarya_kulkarni/tensorflow-test/env/bin/python3.8"
virtualenv_create("r-reticulate", python = path_to_python)
install_tensorflow(envname = "r-reticulate")
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(tensorflow)
library(reticulate)
path_to_python <- "/Users/aarya_kulkarni/tensorflow-test/env/lib/python3.8"
virtualenv_create("r-reticulate", python = path_to_python)
install_tensorflow(envname = "r-reticulate")
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(tensorflow)
library(reticulate)
path_to_python <- "/Users/aarya_kulkarni/tensorflow-test/env/lib/python3.8"
virtualenv_create("r-reticulate", python = path_to_python)
library(keras)
library(tensorflow)
library(reticulate)
library('keras'); reticulate::install_python(version = '3.10');install_keras()
library(tensorflow)
path_to_python <- "/Users/aarya_kulkarni/tensorflow-test/env/lib/python3.8"
virtualenv_create("r-reticulate", python = path_to_python)
tf$constant("Hello TensorFlow!")
knitr::opts_chunk$set(echo = TRUE)
install_tensorflow(envname = "r-reticulate")
install_keras(envname = "r-reticulate")
